---
layout: post

title: "Neur2SP: Neural Two-Stage Stochastic Programming By Elias B.Khalil"

subtitle: ""

header-img: "img/2023/5/Neur2SP.png"

header-mask: 0.2

published: true

tags:

- L20

---

# Note for Neur2SP: Neural Two-Stage Stochastic Programming

## Introduction

### Two-stage Stochastic Programming 

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522103622929.png" alt="image-20230522103622929" style="zoom:50%;" />

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522105437571.png" alt="image-20230522105437571" style="zoom:50%;" />

One of the biggest challenge for solving the two-stage stochastic programming is to evaluate the expected second stage cost, where the second stage decision may involve some uncertainty. It is oftenly used to model the real problem like the following:

- make the first stage decision
- a given scienario happens
- decide the second stage variable
- the objecitve in all is to minimize the cost of the first stage decision and the expected cost for the second stage decision (since we have uncertainty involved, and before thing happening, we can only evaluate the expectation)

**Remark**: Recourse is the ability to take corrective action after a random event has taken place.

### Sample Average Approximation

Since evaluting the expectation of the second stage function is intractable, people usually use the method Sample Average Approximation to approximate it.

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522105616541.png" alt="image-20230522105616541" style="zoom:50%;" />

### Extended Form

We can explicitly writing out the second stage decision variable, then the 2SP looks like a nested optimization problem. And we can observe that when $ K $ is fixed, 

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522105848191.png" alt="image-20230522105848191" style="zoom:45%;" />

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522151559539.png" alt="image-20230522151559539" style="zoom:50%;" />

- $n_1$ : dimension of the first-stage cost
- $n_2$ : dimension of the second-stage cost
- each second stage variable only couple with the first-stage variable, and independ with other second-stage variables
- The extensive form grows linearly with the number $K$

## Supervised Learning for 2SP

### Per-Scenario

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522152453692.png" alt="image-20230522152453692" style="zoom:50%;" />

Learn a function $\tilde{Q}$ taking first-stage variable and scenario as input $(x, \xi_k)$ to approximate the second-stage value function. The NN can only involve the ReLU activation function to be able to be transformed into a MIP. More detailed can be found here.https://link.springer.com/article/10.1007/s10601-018-9285-6

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522152809013.png" alt="image-20230522152809013" style="zoom:50%;" />

After training the NN for the second-stage cost, we can MIPify it in to the original objective function, (which result in a MIP and expected to be much easier to solve)

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522152925341.png" alt="image-20230522152925341" style="zoom:27%;" />

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522153505597.png" alt="image-20230522153505597" style="zoom:27%;" />

We can notice that the resulting MIP will becom intractable if $K$ is relative large, similar to the EF.

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522153757774.png" alt="image-20230522153757774" style="zoom:37%;" />

### Expected Value

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522153905415.png" alt="image-20230522153905415" style="zoom:35%;" />

Therefore, instead of taking a single $(x, \xi_k)$ as input and predict the second-stage cost for each scenario, now, we want to train a new network that can take the first-stage variable and all $K$ scenarios as input and directly predict the sum of the second-stage costs for the $K$ scenarios. 

Then the next problem is how should we encode the $K$ scenarios to the network

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522154352542.png" alt="image-20230522154352542" style="zoom:37%;" />

- The entire Network is trained in an end-to-end fashion, however, only the “ReLU Network” will be embedded into the MIP-NN (that is to solve for the first-stage variable)
- The scenario embedding mainly contains the following three parts:
  - embedding each scenario into the embedding space 
  - aggragate the $K$ embedded vectors into a single vector
  - embedding the aggregated vector into the final embedding space
- The network only used ReLU as the activation function with dropout if specified. *In this sense, we can definitely try some more advance embedding network for better representations.*

## Implementation

### Data Generation

- Generating the first-stage costs and capacities
- generate scenarios by sampling $K$ demand vectors
- To ensure relatively complete recourse, we introduce additional variables with prohibitively expensive objective costs in the case where customers cannot be served

### Evaluation

- solve MIP-NN to get the final solution and objective
- True obj: 
  - fixed the first-stage solution to the second-stage subproblem
  - solve each second-stage problem under each scenario with equal probability
  - Fixed_cost $\times$ first_stage_sol + sum_of_the_second_stage_cost

## Thinking

By using this method, we can efficiently get a high quality first-stage solution. However, in order to get the second-stage variables under different scenarios, we still need to solve the $K$ second-stage subproblems, which can be easily parallized. Therefore, the objective value obtained from solving the MIP-NN may not corresponding to a real combination of first-stage and second-stage variables, and may not even be infomative (providing lower or upper bound) since it is based on a data driven method. 

Several other things:

- predict the solution may be hard for both integer and continous programming, however, predicting the objective value may be relatively easy since we are not learning a map to a high dimensional space but only a single real value. This matches our objective to find a surrogate function to the second-stage cost by directly predict its value instead of finding the solutions. 
- For the 2SP, in most cases, the main objective is to efficiently find out a high quality first-stage solution. Here we assume that after subsituting the second-stage cost, the master problem can be easily solved by the state-of-the-art MIP solvers. Luckily this assumption holds in most cases in practice.
- One limitation is that in order to MIPify the NN, we can only using ReLU and dropout in the network.
- The embedding part should be able to change. 
- How to fine-tune the model to better approximate the objective function, also equivalent to how to let the NN predict a seasonal second stage cost. 

## Reference

- https://khalil-research.github.io/Neur2SP/
- 