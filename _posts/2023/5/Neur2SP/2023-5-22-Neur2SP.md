---
layout: post

title: "Neur2SP"

subtitle: ""

header-img: "img/2023/5/Neur2SP.png"

header-mask: 0.2

published: true

tags:

- L20

---

# Note for Neur2SP: Neural Two-Stage Stochastic Programming

## Introduction

### Two-stage Stochastic Programming 

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522103622929.png" alt="image-20230522103622929" style="zoom:50%;" />

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522105437571.png" alt="image-20230522105437571" style="zoom:50%;" />

One of the biggest challenge for solving the two-stage stochastic programming is to evaluate the expected second stage cost, where the second stage decision may involve some uncertainty. It is oftenly used to model the real problem like the following:

- make the first stage decision
- a given scienario happens
- decide the second stage variable
- the objecitve in all is to minimize the cost of the first stage decision and the expected cost for the second stage decision (since we have uncertainty involved, and before thing happening, we can only evaluate the expectation)

**Remark**: Recourse is the ability to take corrective action after a random event has taken place.

### Sample Average Approximation

Since evaluting the expectation of the second stage function is intractable, people usually use the method Sample Average Approximation to approximate it.

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522105616541.png" alt="image-20230522105616541" style="zoom:50%;" />

### Extended Form

We can explicitly writing out the second stage decision variable, then the 2SP looks like a nested optimization problem. And we can observe that when $ K $ is fixed, 

<img src="https://github.com/xiong-p/xiong-p.github.io/raw/master/_posts/2023/5/Neur2SP/assets/image-20230522105848191.png" alt="image-20230522105848191" style="zoom:45%;" />

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522151559539.png" alt="image-20230522151559539" style="zoom:50%;" />

- $n_1$ : dimension of the first-stage cost
- $n_2$ : dimension of the second-stage cost
- each second stage variable only couple with the first-stage variable, and independ with other second-stage variables
- The extensive form grows linearly with the number $K$

## Supervised Learning for 2SP

### Per-Scenario

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522152453692.png" alt="image-20230522152453692" style="zoom:50%;" />

Learn a function $\tilde{Q}$ taking first-stage variable and scenario as input $(x, \xi_k)$ to approximate the second-stage value function. The NN can only involve the ReLU activation function to be able to be transformed into a MIP. More detailed can be found here.https://link.springer.com/article/10.1007/s10601-018-9285-6

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522152809013.png" alt="image-20230522152809013" style="zoom:50%;" />

After training the NN for the second-stage cost, we can MIPify it in to the original objective function, (which result in a MIP and expected to be much easier to solve)

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522152925341.png" alt="image-20230522152925341" style="zoom:27%;" />

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522153505597.png" alt="image-20230522153505597" style="zoom:27%;" />

We can notice that the resulting MIP will becom intractable if $K$ is relative large, similar to the EF.

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522153757774.png" alt="image-20230522153757774" style="zoom:37%;" />

### Expected Value

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522153905415.png" alt="image-20230522153905415" style="zoom:35%;" />

Therefore, instead of taking a single $(x, \xi_k)$ as input and predict the second-stage cost for each scenario, now, we want to train a new network that can take the first-stage variable and all $K$ scenarios as input and directly predict the sum of the second-stage costs for the $K$ scenarios. 

Then the next problem is how should we encode the $K$ scenarios to the network

<img src="/Users/xiongjinxin/A-xjx/blog/xiong-p.github.io/_posts/2023/5/Neur2SP/assets/image-20230522154352542.png" alt="image-20230522154352542" style="zoom:37%;" />

- The entire Network is trained in an end-to-end fashion, however, only the “ReLU Network” will be embedded into the MIP-NN (that is to solve for the first-stage variable)
- The scenario embedding mainly contains the following three parts:
  - embedding each scenario into the embedding space 
  - aggragate the $K$ embedded vectors into a single vector
  - embedding the aggregated vector into the final embedding space
- The network only used ReLU as the activation function with dropout if specified. *In this sense, we can definitely try some more advance embedding network for better representations.*



## Reference

- https://khalil-research.github.io/Neur2SP/
- 